---
title: "Regression Analysis"
output: html_document
---

<br>

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(performance)
library(modelr)
library(mgcv)
library(corrplot)
library(leaps)
library(glmnet)
library(rsample)
library(caret)
library(patchwork)
library(Metrics)
library(MASS)
library(glmnet)

knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 6,
  fig.asp = .7,
  dpi = 800,
  out.width = "80%"
)

theme_set(theme_minimal())

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


```{r include=FALSE, message=FALSE}
le_df = 
  read_csv("./data/Dummy_Data.CSV")
```

<br>

## Correlation Matrix

```{r}
cm_df =
  le_df %>%
  dplyr::select(-c(country,continent,year))

corrplot(cor(cm_df), type = "upper", diag = FALSE, method = "square", addCoef.col = "black", number.cex = .3, tl.col = "black", tl.cex = .7)
```

<br>

<hr>

## Model building 

### Model Based on Prior Research

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Developed \ Status) + \beta_{2}*(Percent \ Expenditure) + \beta_{3}*(HDI) + \beta_{4}*(Adult \ Mortality) + \beta_{5}*(Infant \ Deaths) + \beta_{6}*(Schooling) + \beta_{7}*(HIV/AIDS) + \beta_{8}*(GDP * Percent \ Expenditure)$
</center>

```{r}
mod_1 = lm(le ~ status_Developed + percent_exp + HDI + adult_mort + infantdeaths + schooling + gdp + hiv_aids + gdp * percent_exp, data = le_df)
mod_1_vif = check_collinearity(mod_1) 
summary(mod_1)
```


<br>
<br>

### Models Based on Automated Model Selection

```{r include = FALSE}
full_model = lm(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high  + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)
```

Full Model:

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Developed \ Status) + \beta_{2}*(Adult \ Mortality) + \beta_{3}*(Infant \ Deaths) + \beta_{4}*(Alcohol) + \beta_{5}*(Percent \ Expenditure) + \beta_{6}*(High \ Hepatitis \ B) + \beta_{7}*(Measles) + \beta_{8}*(BMI) + \beta_{9}*(Under-five \ Deaths) + \beta_{10}*(High \ Polio) + \beta_{11}*(Total \ Expenditure) + \beta_{12}*(High \ Diphtheria) + \beta_{13}*(HIV/AIDS) + \beta_{14}*(GDP) + \beta_{15}*(Population) + \beta_{16}*(Prevalence \ of \ thinness 10-19) + \beta_{17}*(Prevalence \ of \ thinness 5-9) + \beta_{18}*(HDI) + \beta_{19}*(Schooling)$
</center>

**Stepwise Regression:**

```{r}
step_model = stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)
check_collinearity(step_model)
```

*Final Stepwise Model*
```{r}
## remove insignificant and high correlation variables (under_five, infantdeaths)
mod_2 = lm(le ~ status_Developed + adult_mort + bmi + polio_high + diphtheria_high + hiv_aids + gdp + thin_1_19 + HDI + schooling, data = le_df)
```

Final Stepwise Model:

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Developed \ Status) + \beta_{2}*(Adult \ Mortality) + \beta_{3}*(BMI) + \beta_{4}*(High \ Polio) + \beta_{5}*(High \ Diphtheria) + \beta_{6}*(HIV/AIDS) + \beta_{7}*(GDP) + \beta_{8}*(Prevalence \ of \ thinness 10-19) + \beta_{9}*(HDI) + \beta_{10}*(Schooling)$
</center>

<br>
<br>

**Forward/Backward Selection:**

*Backward Elimination*
```{r}
back_mod = stepAIC(full_model, direction = "backward", 
                      trace = FALSE, scope = formula(full_model))
summary(back_mod)
check_collinearity(back_mod)

backward = lm(le ~ status_Developed + adult_mort + bmi + polio_high + diphtheria_high + hiv_aids + gdp + thin_1_19 + HDI + schooling, data = le_df)
```

*Forward Selection*
```{r}
forward_mod = stepAIC(full_model, direction = "forward", trace = FALSE, scope = formula(full_model))
summary(forward_mod)
check_collinearity(forward_mod)

forward = lm(le ~ status_Developed + adult_mort + infantdeaths + bmi + polio_high + diphtheria_low + hiv_aids + gdp + HDI + schooling, data = le_df)
```

<br>
<br>


**Best Subsets Regression:**

<br>

*Best Fitting Models Based on the Numbers of Predictors*
```{r include=FALSE}
sub_fit = regsubsets(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high  + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)

best_summary = summary(sub_fit)
best_summary_df = 
  as.data.frame(best_summary$outmat) %>% 
  remove_rownames() %>% 
  add_column(" " = c(1:8), .before = "status_Developed")

table = as.data.frame(t(best_summary_df[-1])) %>% 
  rename("1" = "V1", "2" = "V2", "3" = "V3","4" = "V4", "5" = "V5", "6" = "V6","7" = "V7", "8" = "V8")
```

```{r}
table %>% 
     knitr::kable()
```

<br>

*Comparison Based on Model Selection Criteria*
```{r}
Model_comparison =
  tibble(
	n_pred = c(1:8),
	"Adjusted R2" = best_summary$adjr2,
	"Adj R2" = best_summary$adjr2,
	"BIC" = best_summary$bic,
	"Cp" = best_summary$cp,
	"RSE" = best_summary$rsq,
	"RSS" = best_summary$rss
	) %>%
	gather(key = "Statistics", value = "value", 2:6) %>%
	ggplot(aes(x = n_pred, y = value)) +
	  geom_point() +
	  geom_line() +
	  facet_grid(Statistics ~ ., scales = "free_y") +
	  labs(
	  	x = "Number of Predictors",
	  	y = "Values",
	  	title = "Model Selection Criteria Comparison"
	  )

Model_comparison
```

*Final Best Subsets Model*
```{r}
mod_3 = lm(le ~ adult_mort + under_five + polio_high + hiv_aids + gdp + HDI + schooling, data = le_df)
```

Final Best Subsets Model:

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Adult \ Mortality) + \beta_{2}*(Under-five Deaths) + \beta_{3}*(High \ Polio) + \beta_{4}*(HIV/AIDS) + \beta_{5}*(GDP) + \beta_{6}*(HDI) + \beta_{7}*(Schooling)$
</center>

<br>
<br>
<hr>

When developing regression models for our dataset, one problem that rises to our attention is the multicollinearity, our dataset contains multiple predictor variables that are highly correlated to each other, which concerns us that they might not provide unique or independent information in the regression models.Thus, in addition to the multiple linear regression models above, we also applied regularization method to resolve multicollinearity issues using ridge regression and lasso regression.

**Ridge Regression:**

Mathematical equation of Ridge Regression: 

$\sum_{i=1}^n{(y_{i} - \beta_{0} - \sum_{j=1}^{p}{\beta_{i}x_{ij}})^2} + \lambda \sum_{j=1}^{p}{\beta_{j}^2} = RSS + \lambda \sum_{j=1}^{p}{\beta_{j}^2}$

```{r, message=FALSE}
#define response variable
y = le_df$le

#define matrix of predictor variables
x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'measles', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'population', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])

#fit ridge regression model
ridge_model = glmnet(x, y, alpha = 0)
  
#perform k-fold cross-validation to find optimal lambda value
cv_ridge_model = cv.glmnet(x, y, alpha = 0)

best_lambda = cv_ridge_model$lambda.min

best_ridge_model = glmnet(x, y, alpha = 0, lambda = best_lambda)

par(mfrow=c(1,1))
#produce plot of test MSE by lambda value
plot(cv_ridge_model) + title("Plot of MSE by Lambda value",line = 2.5)
```
* The lambda value that minimizes the test MSE turns out to be `r best_lambda`.

<br>
<br>

**Lasso Regression:**

Mathematical equation of Ridge Regression: 

$\sum_{i=1}^n{(y_{i} - \beta_{0} - \sum_{j=1}^{p}{\beta_{i}x_{ij}})^2} + \lambda \sum_{j=1}^{p}{|\beta_{j}|} = RSS + \lambda \sum_{j=1}^{p}{|\beta_{j}|}$

```{r, message=FALSE}
#define response variable
y = le_df$le

#define matrix of predictor variables
x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'measles', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'population', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])

lasso_model = glmnet(x, y, alpha = 1)

#perform k-fold cross-validation to find optimal lambda value
cv_lasso_model = cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lasso_lambda = cv_lasso_model$lambda.min

best_lasso_model = glmnet(x, y, alpha = 1, lambda = best_lasso_lambda)

new_x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])
```

* The lambda value that minimizes the test MSE turns out to be `r best_lasso_lambda`.

```{r}
#produce plot of test MSE by lambda value
plot(cv_lasso_model) + title("Plot of MSE by Lambda value",line = 2.5)

coef(best_lasso_model)
```

* The coefficients of the predictor `measles` and `population` was shrunk all the way to zero, which means it was completely removed from the model since it wasnâ€™t influential.

<br>
<br>
<hr>

## Cross Validation

```{r include=FALSE}
set.seed(2)

cv_df = 
  crossv_mc(le_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    mod_1 = map(train, ~lm(formula = formula(mod_1), data = .x)),
    mod_2 = map(train, ~lm(formula = formula(mod_2), data = .x)),
    mod_3 = map(train, ~lm(formula = formula(mod_3), data = .x)),
    
    mod_4 = map(train, ~train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 0, lambda = best_lambda)), data = .x),
    
    mod_5 = map(train, ~train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 1, lambda = best_lasso_lambda)), data = .x)) %>% 
  
  mutate(
    rmse_1 = map2_dbl(mod_1, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(mod_2, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(mod_3, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_4 = map2_dbl(mod_4, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_5 = map2_dbl(mod_5, test, ~modelr::rmse(model = .x, data = .y))
    ) %>% 
  mutate(
    mae_1 = map2_dbl(mod_1, test, ~modelr::mae(model = .x, data = .y)),
    mae_2 = map2_dbl(mod_2, test, ~modelr::mae(model = .x, data = .y)),
    mae_3 = map2_dbl(mod_3, test, ~modelr::mae(model = .x, data = .y)),
    mae_4 = map2_dbl(mod_4, test, ~modelr::mae(model = .x, data = .y)),
    mae_5 = map2_dbl(mod_5, test, ~modelr::mae(model = .x, data = .y))
    ) %>% 
  mutate(
    rsq_1 = map2_dbl(mod_1, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_2 = map2_dbl(mod_2, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_3 = map2_dbl(mod_3, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_4 = map2_dbl(mod_4, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_5 = map2_dbl(mod_5, test, ~modelr::rsquare(model = .x, data = .y))
    )
```


### Cross-validating with Violin Plots
```{r include=FALSE}
rmse_table = 
  cv_df %>%
  pivot_longer(
    rmse_1:rmse_5,
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Model 1 (Research)" = "1",
                                   "Model 2 (Stepwise)" = "2",
                                   "Model 3(Best Subsets)" = "3",
                                   "Model 4 (Ridge)" = "4",
                                   "Model 5 (Lasso)" = "5" ))

rsq_table = 
  cv_df %>% 
  pivot_longer(
    rsq_1:rsq_5,
    names_to = "model", 
    values_to = "rsq",
    names_prefix = "rsq_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Model 1 (Research)" = "1",
                                   "Model 2 (Stepwise)" = "2",
                                   "Model 3(Best Subsets)" = "3",
                                   "Model 4 (Ridge)" = "4",
                                   "Model 5 (Lasso)" = "5" ))
mae_table = 
  cv_df %>% 
  pivot_longer(
    mae_1:mae_5,
    names_to = "model", 
    values_to = "mae",
    names_prefix = "mae_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Model 1 (Research)" = "1",
                                   "Model 2 (Stepwise)" = "2",
                                   "Model 3(Best Subsets)" = "3",
                                   "Model 4 (Ridge)" = "4",
                                   "Model 5 (Lasso)" = "5" ))
```


```{r}
rmse_plot =
  rmse_table %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "RMSE") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

rsq_plot =
  rsq_table %>% 
  ggplot(aes(x = model, y = rsq, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "RSquared") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

mae_plot =
  mae_table %>% 
  ggplot(aes(x = model, y = mae, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "MAE") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

rmse_plot/rsq_plot/mae_plot
```

<br>
<br>

### Cross Validation Table
```{r message=FALSE}
mean_rmse =
  rmse_table %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse))

mean_mae = 
  mae_table %>%
  group_by(model) %>% 
  summarize(mean_mae = mean(mae))

mean_rsquare = 
  rsq_table %>% 
  group_by(model) %>% 
  summarize(mean_rsq = mean(rsq))

Model_comparison_table =
  full_join(inner_join(mean_rmse, mean_rsquare), mean_mae) %>% 
  knitr::kable()

Model_comparison_table 
```


<br>
<hr>

## Results

Based on performance measures among all models Lasso regression shows least Mean Absolute Error (MAE = 2.873414) and Root Mean Square Error (RMSE = 3.840274), and highest R-square value of 0.8378577. Thus, Lasso regression is considered as best model to predict Life expectancy.

The coefficients for the best model we obtain are as follows: 

```{r}
lasso_coeff <- coef(best_lasso_model)

data.frame(name = lasso_coeff@Dimnames[[1]][lasso_coeff@i + 1], coefficient = lasso_coeff@x) %>% 
  as_tibble()

plot(lasso_model, xvar = "lambda") + title("Lasso trace plot",line = 2.5)
```

```{r}
optimal_model = train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 1, lambda = best_lasso_lambda))

ggplot(varImp(optimal_model)) + 
  labs( 
    title = "Most important variables from elastic CV",
    x = "Predictors") 
      
Lasso_lm <- lm(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)

summary(Lasso_lm)
```

* The most important variables we get from this are under-five deaths, infant deaths, Schooling, HIV/AIDS, Adult mortality, HDI, BMI and High Polio (Percentage of Polio immunization coverage among 1-year-olds $\ge 80$.

* GDP, Developed Status, Diphtheria, Hepatitis B and total expenditure are less important.
Alcohol and Polio are right in the middle of the variable importance graph.

* Not all predictors that the Lasso selects are statistically significant at a 5% level. Thinness..1.19.years, for example, has a p-value of about 0.14.



