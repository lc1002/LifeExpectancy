---
title: "Predictive Model for Life Expectancy"
output: 
  html_document:
    toc: true
    toc_float: true
---

<style type="text/css">

h1.title {
  font-size: 50px;
  text-align: left;   
  font-family: Papyrus;
  font-weight: bold;
  font-variant: small-caps
}
</style>

The main objective of this study is to analyze the relationship between life expectancy and relevant variables that can potentially affect an individual's lifespan. In conjunction with the exploratory and statistical analyses, we will now build a predictive model for life expectancy using existing data of all countries from 2000 to 2015.

In this page, we will first examine the correlation between life expectancy and potential predictors in our dataset using **correlation matrix**. Then we will propose several models based on **prior research** and **automated model selection**. Next, we will evaluate and compare the performance of the models with metrics of evaluation using **cross-validation**. Finally, we will report the **results** and draw a conclusion based on our best-fitted model.

<br>
<br>
<hr>

```{r include=FALSE, message=FALSE}
library(tidyverse)
library(performance)
library(see)
library(modelr)
library(mgcv)
library(corrplot)
library(leaps)
library(glmnet)
library(rsample)
library(caret)
library(patchwork)
library(Metrics)
library(MASS)
library(glmnet)

knitr::opts_chunk$set(
  echo = TRUE,
  fig.width = 6,
  fig.asp = .7,
  dpi = 800,
  out.width = "80%"
)

theme_set(theme_minimal())

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


```{r include=FALSE, message=FALSE}
le_df = 
  read_csv("./data/Dummy_Data.CSV")
```

## **Correlation Matrix**

One of the essential steps of building a regression model is to detect multicollinearity between the independent variables. Below is a table that displays correlations between variables in our dataset.

<br>

```{r}
cm_df =
  le_df %>%
  dplyr::select(-c(country,continent,year))

corrplot(cor(cm_df), type = "upper", diag = FALSE, method = "square", addCoef.col = "black", number.cex = .3, tl.col = "black", tl.cex = .7)
```

<br>

From the correlation matrix, we can see that life expectancy has a high positive correlation with `HDI` and `schooling`, and a high negative correlation with `adult_mort` and moderate negative correlation with `hiv_aids`. Indeed,  we would expect the outcome variable, life expectancy, to be highly impacted by these predictors.

We can also see that many of our variables are moderately correlated with each other. `HDI` highly correlated with `schooling` and moderately correlated with multiple variables. It is positively associated with `alcohol`, `percent_exp`, `bmi`, `gdp`, `status_Developing`, `polio_low`,  `hepatitis_b_low` and `diphtheria_low` , and negatively associated with `adult_mort`, `status_Developed`, `thin_1_19`, `thin_5_9`,`polio_high`,  `hepatitis_b_high` and `diphtheria_high`. 

Some of our variables are also highly correlated with each other. For example, `gdp` has a strong positive correlation with `percent_exp`. The variable `infant deaths` and `under-five deaths` are highly correlated. 
Furthermore, `thin_1_19` and `thin_5_9` also have strong correlation with each other. In addition, dummy variables created from the same categorical variable are highly correlated as well (e.g. `status_Developed` and `status_Developing`). Therefore, multicollinearity must be taken into consideration when selecting our model.

(For a detailed analysis of the predictors in our dataset, please visit [statistical analysis]( https://lc1002.github.io/LifeExpectancy/statistical.html) page.)


<br>
<br>
<hr>

## **Model building** 

Variable selection is an essential aspect of statistical model building. Carefully selecting the truly predictive variables from a large pool of candidate predictors to build a predictive model with minimal prediction error, is often considered the most difficult step of model building. To find the best-fitted model for life expectancy prediction, we have proposed several models based on prior research and automatic model selection.

<br>

### Model Based on Prior Research

Theory and findings from relevant research can provide good guidance when deciding which variables to include in the model. 

Studies have shown that socioeconomic developments and health measures are associated with life expectancy. Lin et al. have conducted a [study]( https://bmcpublichealth.biomedcentral.com/articles/10.1186/1471-2458-12-85) to examine the longitudinal contributions of four socioeconomic indicators, economy, educational environment, nutritional status, and political regime, in less developed countries. The results of the study proven that the increases in life expectancy of less developed countries over time were associated with all four factors.(Lin et al., 2012) Another research from [Kruk et al., 2018]( https://www.sciencedirect.com/science/article/pii/S0140673618316684) shows that 58% of all amenable mortality in low-income and middle-income countries are attributed to low-quality health system. Indeed, using mortality rates as health measures, we see that low-quality health system was associated with the decrease of life expectancy in these countries. Although these studies were only conducted in less developed countries, we believe that the results are still good indicators of the determinants of life expectancy.

Based on prior research and analyses, we proposed the following model using socioeconomic and health-related variables from our dataset:

<br>

**Research Model:**

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Developed \ Status) + \beta_{2}*(Percent \ Expenditure) + \beta_{3}*(HDI) + \beta_{4}*(Adult \ Mortality) + \beta_{5}*(Infant \ Deaths) + \beta_{6}*(Schooling) + \beta_{7}*(HIV/AIDS) + \beta_{8}*(GDP * Percent \ Expenditure)$
</center>

```{r include=FALSE}
mod_1 = lm(le ~ status_Developed + percent_exp + HDI + adult_mort + infantdeaths + schooling + gdp + hiv_aids + gdp * percent_exp, data = le_df)
mod_1_vif = check_collinearity(mod_1) 
summary(mod_1)
```


<br>
<br>

### Models Based on Automated Model Selection

Automated selection procedures are often used when there are many potential variables available in the data, and there are uncertainties in selecting the most appropriate variables for a reliable prediction. Using automated model selection performed in R, we have generated four models. Outlined below are the four automated model selection procedures performed: **stepwise selection**,**backward/forward selection**, **best subsets selection**, **lasso regression** and **ridge regression**.

```{r include=FALSE}
full_model = lm(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high  + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)
```

<br>

#### **Stepwise Regression:**

Stepwise model selection was performed using `stepAIC()` from the `MASS` package. High correlation variables and insignificant variables were detected using `check.collinearity()` from the `performance` package and `summary()` from base R, respectively. Below is the final model obtained using stepwise selection.

<br>

```{r include = FALSE}
step_model = stepAIC(full_model, direction = "both", trace = FALSE)
summary(step_model)
check_collinearity(step_model)

## remove insignificant and high correlation variables (under_five, infantdeaths)
mod_2 = lm(le ~ status_Developed + adult_mort + bmi + polio_high + diphtheria_high + hiv_aids + gdp + thin_1_19 + HDI + schooling, data = le_df)
```

**Stepwise Model:**

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Developed \ Status) + \beta_{2}*(Adult \ Mortality) + \beta_{3}*(BMI) + \beta_{4}*(High \ Polio) + \beta_{5}*(High \ Diphtheria) + \beta_{6}*(HIV/AIDS) + \beta_{7}*(GDP) + \beta_{8}*(Prevalence \ of \ thinness 10-19) + \beta_{9}*(HDI) + \beta_{10}*(Schooling)$

</center>

<br>
<br>

#### **Forward/Backward Selection:**

Backward elimination was generated using `stepAIC()` with `direction = backward`, from the `MASS` package. Similarly, forward selection was generated using `stepAIC()` with `direction = forward`. Then, high correlation and insignificant variables were detected and removed from the resulted models. Surprisingly, after removing these variables, we obtained two identical models from the backward and forward selection, which is also the exact same model generated from the stepwise selection. 

Since models produced by backward, forward and stepwise selection are identical, we will only include one of them in further analysis. For simplicity, we will label this model as stepwise model.

```{r include = FALSE}
back_mod = stepAIC(full_model, direction = "backward", 
                      trace = FALSE, scope = formula(full_model))
summary(back_mod)
check_collinearity(back_mod)

backward = lm(le ~ status_Developed + adult_mort + bmi + polio_high + diphtheria_high + hiv_aids + gdp + thin_1_19 + HDI + schooling, data = le_df)

forward_mod = stepAIC(full_model, direction = "forward", trace = FALSE, scope = formula(full_model))
summary(forward_mod)
check_collinearity(forward_mod)

forward = lm(le ~ status_Developed + adult_mort + infantdeaths + bmi + polio_high + diphtheria_low + hiv_aids + gdp + HDI + schooling, data = le_df)
```

<br>

#### **Best Subsets Regression:**

Best subset regression was performed using `regsubsets()` from the `leaps` package. Unlike stepwise, forward, and backward selection that generates only one regression model, best subset regression produces multiple models after fitting all models based on the number of independent variables specified.

Below is a table that displays the best fitting models generated from best subset regression of different sizes.

<br>

#### *Best Fitting Models Based on the Numbers of Predictors*
```{r include=FALSE}
sub_fit = regsubsets(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high  + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)

best_summary = summary(sub_fit)
best_summary_df = 
  as.data.frame(best_summary$outmat) %>% 
  remove_rownames() %>% 
  add_column(" " = c(1:8), .before = "status_Developed")

table = as.data.frame(t(best_summary_df[-1])) %>% 
  rename("1" = "V1", "2" = "V2", "3" = "V3","4" = "V4", "5" = "V5", "6" = "V6","7" = "V7", "8" = "V8")
```

```{r}
table %>% 
     knitr::kable()
```

<br>

Since best subset regression did not specify the best model among the models generated, we will need to compare the model selection criteria to determine the final best subset model. Here is a plot that compares the model selection criteria across all models generated from the best subset regression:

<br>

#### *Comparison Based on Model Selection Criteria*
```{r}
Model_comparison =
  tibble(
	n_pred = c(1:8),
	"Adj R2" = best_summary$adjr2,
	"BIC" = best_summary$bic,
	"Cp" = best_summary$cp,
	"RSE" = best_summary$rsq,
	"RSS" = best_summary$rss
	) %>%
	gather(key = "Statistics", value = "value", 2:6) %>%
	ggplot(aes(x = n_pred, y = value)) +
	  geom_point() +
	  geom_line() +
	  facet_grid(Statistics ~ ., scales = "free_y") +
	  labs(
	  	x = "Number of Predictors",
	  	y = "Values",
	  	title = "Model Selection Criteria Comparison"
	  )

Model_comparison
```

<br>

Based on the results shown above, the model contains 8 predictors has the highest adjusted R squared and relative standard error (RSE), and the lowest bayesian Information criterion (BIC), Mallow’s Cp value, and residual sum of squares (RSS). Therefore, it is the best-fitting best subset model among all candidate models.

After removing high correlation variables from the resulted models, we obtained our final best subsets model. 

<br>

```{r include=FALSE}
mod_3 = lm(le ~ adult_mort + under_five + polio_high + hiv_aids + gdp + HDI + schooling, data = le_df)
```

**Best Subsets Model:**

<center>
$Life \ Expectancy = \beta_{0} + \beta_{1}*(Adult \ Mortality) + \beta_{2}*(Under-five Deaths) + \beta_{3}*(High \ Polio) + \beta_{4}*(HIV/AIDS) + \beta_{5}*(GDP) + \beta_{6}*(HDI) + \beta_{7}*(Schooling)$
</center>

<br>
<br>

When developing regression models for our dataset, one problem that rises to our attention is the multicollinearity, our dataset contains multiple predictor variables that are highly correlated to each other, which concerns us that they might not provide unique or independent information in the regression models.Thus, in addition to the multiple linear regression models above, we also applied regularization method to resolve multicollinearity issues using ridge regression and lasso regression.

<br>

#### **Ridge Regression:**

**Mathematical equation of Ridge Regression:**

$\sum_{i=1}^n{(y_{i} - \beta_{0} - \sum_{j=1}^{p}{\beta_{i}x_{ij}})^2} + \lambda \sum_{j=1}^{p}{\beta_{j}^2} = RSS + \lambda \sum_{j=1}^{p}{\beta_{j}^2}$

```{r, message=FALSE}
#define response variable
y = le_df$le

#define matrix of predictor variables
x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'measles', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'population', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])

#fit ridge regression model
ridge_model = glmnet(x, y, alpha = 0)
  
#perform k-fold cross-validation to find optimal lambda value
cv_ridge_model = cv.glmnet(x, y, alpha = 0)

best_lambda = cv_ridge_model$lambda.min

best_ridge_model = glmnet(x, y, alpha = 0, lambda = best_lambda)

par(mfrow=c(1,1))
#produce plot of test MSE by lambda value
plot(cv_ridge_model) + title("Plot of MSE by Lambda value",line = 2.5)
```
The lambda value that minimizes the test MSE turns out to be `r best_lambda`.

<br>
<br>

#### **Lasso Regression:**

**Mathematical equation of Ridge Regression:**

$\sum_{i=1}^n{(y_{i} - \beta_{0} - \sum_{j=1}^{p}{\beta_{i}x_{ij}})^2} + \lambda \sum_{j=1}^{p}{|\beta_{j}|} = RSS + \lambda \sum_{j=1}^{p}{|\beta_{j}|}$

```{r, message=FALSE}
#define response variable
y = le_df$le

#define matrix of predictor variables
x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'measles', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'population', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])

lasso_model = glmnet(x, y, alpha = 1)

#perform k-fold cross-validation to find optimal lambda value
cv_lasso_model = cv.glmnet(x, y, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lasso_lambda = cv_lasso_model$lambda.min

best_lasso_model = glmnet(x, y, alpha = 1, lambda = best_lasso_lambda)

new_x = data.matrix(le_df[, c('status_Developed', 'adult_mort', 'infantdeaths', 'alcohol', 'percent_exp', 'hepatitis_b_high', 'bmi', 'under_five', 'polio_high', 'total_exp', 'diphtheria_high', 'hiv_aids', 'gdp', 'thin_1_19', 'thin_5_9', 'HDI', 'schooling')])
```

The lambda value that minimizes the test MSE turns out to be `r best_lasso_lambda`.

```{r}
#produce plot of test MSE by lambda value
plot(cv_lasso_model) + title("Plot of MSE by Lambda value",line = 2.5)

coef(best_lasso_model) 
```

The coefficients of the predictor `measles` and `population` was shrunk all the way to zero, which means it was completely removed from the model since it wasn’t influential.

<br>
<br>
<hr>

## **Cross Validation**

Cross-validation is a statistical model evaluation method used to compare the prediction accuracy of competing models using the regression model accuracy metrics. In this section, we will use the `modelr` package to perform the cross validation process and compare the regression model accuracy metrics, including the root mean square deviation (RMSE), R-squared (R^2), and mean absolute error (MAE), across all models proposed from the **Model building** section above. 

First, we will use `crossv_mc()` to generate 100 random partitions by performing the training/testing split (i.e., 80% of the random partitions will be the training datasets, and 20% will be the testing datasets). Then, we will fit all candidate models to each of our training data and obtain the corresponding RMSE, R^2 and MAE for the testing data using `map()` and `map2_dbl()`. The corresponding RMSE, R^2 and MAE will be used as a measure of our model's prediction accuracy on the testing dataset. Finally, we will visualize the distribution of RMSE, R^2 and MAE of all models using `geom_violin()` from `ggplot`, and compare the performance of the candidate models based on the regression model accuracy metrics.


```{r}
set.seed(2)

cv_df = 
  crossv_mc(le_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  ) %>% 
  mutate(
    mod_1 = map(train, ~lm(formula = formula(mod_1), data = .x)),
    mod_2 = map(train, ~lm(formula = formula(mod_2), data = .x)),
    mod_3 = map(train, ~lm(formula = formula(mod_3), data = .x)),
    mod_4 = map(train, ~train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + measles + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + population + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 0, lambda = best_lambda)), data = .x),
    mod_5 = map(train, ~train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 1, lambda = best_lasso_lambda)), data = .x)) %>% 
  mutate(
    rmse_1 = map2_dbl(mod_1, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(mod_2, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(mod_3, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_4 = map2_dbl(mod_4, test, ~modelr::rmse(model = .x, data = .y)),
    rmse_5 = map2_dbl(mod_5, test, ~modelr::rmse(model = .x, data = .y))
    ) %>% 
  mutate(
    mae_1 = map2_dbl(mod_1, test, ~modelr::mae(model = .x, data = .y)),
    mae_2 = map2_dbl(mod_2, test, ~modelr::mae(model = .x, data = .y)),
    mae_3 = map2_dbl(mod_3, test, ~modelr::mae(model = .x, data = .y)),
    mae_4 = map2_dbl(mod_4, test, ~modelr::mae(model = .x, data = .y)),
    mae_5 = map2_dbl(mod_5, test, ~modelr::mae(model = .x, data = .y))
    ) %>% 
  mutate(
    rsq_1 = map2_dbl(mod_1, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_2 = map2_dbl(mod_2, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_3 = map2_dbl(mod_3, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_4 = map2_dbl(mod_4, test, ~modelr::rsquare(model = .x, data = .y)),
    rsq_5 = map2_dbl(mod_5, test, ~modelr::rsquare(model = .x, data = .y))
    )
```

<br>

### *Cross-validating with Violin Plots*
```{r include=FALSE}
rmse_table = 
  cv_df %>%
  pivot_longer(
    rmse_1:rmse_5,
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Research Model" = "1",
                                   "Stepwise Model" = "2",
                                   "BestSub Model" = "3",
                                   "Ridge Model" = "4",
                                   "Lasso Model" = "5"))

rsq_table = 
  cv_df %>% 
  pivot_longer(
    rsq_1:rsq_5,
    names_to = "model", 
    values_to = "rsq",
    names_prefix = "rsq_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Research Model" = "1",
                                   "Stepwise Model" = "2",
                                   "BestSub Model" = "3",
                                   "Ridge Model" = "4",
                                   "Lasso Model" = "5" ))
mae_table = 
  cv_df %>% 
  pivot_longer(
    mae_1:mae_5,
    names_to = "model", 
    values_to = "mae",
    names_prefix = "mae_") %>% 
  mutate(model = fct_inorder(model),
         model = fct_recode(model, "Research Model" = "1",
                                   "Stepwise Model" = "2",
                                   "BestSub Model" = "3",
                                   "Ridge Model" = "4",
                                   "Lasso Model" = "5" ))
```


```{r}
rmse_plot =
  rmse_table %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "RMSE") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

rsq_plot =
  rsq_table %>% 
  ggplot(aes(x = model, y = rsq, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "RSquared") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

mae_plot =
  mae_table %>% 
  ggplot(aes(x = model, y = mae, fill = model)) + 
  geom_violin() + 
  labs(x = "",
       y = "MAE") +
  scale_fill_brewer(type = 'seq', palette = 'Set2') +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

rmse_plot/rsq_plot/mae_plot
```

<br>

In addition, we also create a table that displays the mean RMSE, mean R^2 and mean MAE of all models.

<br>

### *Cross Validation Table*
```{r message=FALSE}
mean_rmse =
  rmse_table %>% 
  group_by(model) %>% 
  summarize(mean_rmse = mean(rmse))

mean_mae = 
  mae_table %>%
  group_by(model) %>% 
  summarize(mean_mae = mean(mae))

mean_rsquare = 
  rsq_table %>% 
  group_by(model) %>% 
  summarize(mean_rsq = mean(rsq))

Model_comparison_table =
  full_join(inner_join(mean_rmse, mean_rsquare), mean_mae) %>% 
  knitr::kable()

Model_comparison_table 
```

<br>
<hr>

## **Results**

Based on performance measures among all models Lasso regression shows least Mean Absolute Error (MAE = 2.873414) and Root Mean Square Error (RMSE = 3.840274), and highest R-square value of 0.8378577. Thus, Lasso regression is considered as the best model to predict Life expectancy.

```{r}
plot(lasso_model, xvar = "lambda") + title("Lasso trace plot",line = 2.5)
```
The trace plot illustrates how the predictors go into the model in the order of their magnitude of true linear regression coefficients.The coefficient path of each variables with respect to $\sum_{i} |\beta_{i}|$ only changes when a new predictor enters the model. When a new predictor enters the model, it affects the slope of the coefficient path of all predictors that are already in the model in a deterministic way as shown. 


The coefficients for the best model we obtain from lasso regression are as follows: 
```{r}
lasso_coeff <- as.data.frame(as.matrix(coef(best_lasso_model))) %>% 
  knitr::kable()

Lasso_lm <- glm(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = le_df)
summary(Lasso_lm) 
```

When we compare the coefficients with the OLS, the coefficients are pretty similar since the penalization, $\lambda$, was low.

```{r}
optimal_model = train(le ~ status_Developed + adult_mort + infantdeaths + alcohol + percent_exp + hepatitis_b_high + bmi + under_five + polio_high + total_exp + diphtheria_high + hiv_aids + gdp + thin_1_19 + thin_5_9 + HDI + schooling, data = training(initial_split(le_df, prop = 0.75))[,-1], trControl=trainControl(method="cv", number = 5), method="glmnet", preProcess = c("center", "scale"),tuneGrid = expand.grid(alpha = 1, lambda = best_lasso_lambda))

ggplot(varImp(optimal_model)) + 
  labs( 
    title = "Most important variables from elastic CV",
    x = "Predictors") 
```

From the plot, the most important variables we get are `under-five deaths`, `infant deaths`, `Schooling`, `HIV/AIDS`, `Adult mortality`, `HDI`, `BMI`, and `High Polio` (Percentage of Polio immunization coverage among 1-year-olds $\ge 80$. GDP, Diphtheria, Development Status and Polio are next influential predictors according to the variable importance graph. Lastly, `Thin_1_9`, `Percent_exp`, `Total_exp`, `Thin_5_9`, `Hepatitis B` and `Alcohol` are less important.

## **Diagnostics**

In order to validate our model and assess its goodness of fit, we perform regression diagnostics to evaluate our model assumptions of ordinary least squares (OLS).

```{r, message=FALSE, warning=FALSE}
check_model(Lasso_lm, check = c("linearity", "outliers", "qq", "normality"))
```

**Linearity:** If a linear model has been properly specified, the error term accounts for the variation in the dependent variable that the independent variables do not explain, the average value of the error term must equal zero and uncorrelated with the fitted values y^s. The line of best fit of the residuals regressed on the fitted values should have an intercept and slope of zero. The Linearity plot shows the data roughly satisfy this assumption.

**Homoscedasticity:** By definition, OLS regression gives equal weight to all observations, we would expect the variance of the errors should be consistent for all observations (fitted values). From the top-left graph, we can see that the residuals tend to be dispersed about the reference line.

**Normality:** To test the normality of our residuals, looking at the bottom left plot which illustrates a strong linear trend. We can say that the residuals are roughly normal, but with thick tails on both end. In addition, the distribution of the residuals in the bottom-right graph closely follows a normal distribution centered at zero. Thus, the normality assumption is satisfy.

**Influential Observations/Outliers:** Outliers are unavoidable in large samples, but we checking no single outlier contributes too much variation by itself is crucial. According to the Influential Observations plot, we see that all of the points fall within the contour lines.

Overall, our model seems to be appropriate for the data. 

<br>
<hr>

<center>
<font size="2"> Contributors: AnMei Chen and Lynn Chen</font><br>
<font size="2">The Github repository for this website can be found [here ](https://github.com/lc1002/LifeExpectancy), and the corresponding Github repository for this project is located [here](https://github.com/lc1002/LifeExpectancyAnalysis). </font> 

<img src="image/mailman.png" alt="logo" style="width:30%; height:20%; ">
</center>
